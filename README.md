# Apache Airflow Docker Setup
This repository provides a quick setup for deploying Apache Airflow using Docker Compose. It includes the necessary configuration for running Apache Airflow in a local Docker environment, along with example DAGs for testing.

## Requirements
Before you start, ensure that you have the following installed:

Docker: Install Docker
Docker Compose: Install Docker Compose
Git: Install Git (if not already installed)
Project Structure
The project follows this directory structure:

```
/airflow-project
    ├── /dags                  # Your Airflow DAG files
    │   ├── simple_python_dag.py
    │   └── bash_operator_dag.py
    ├── /logs                  # Logs generated by Airflow (not version-controlled)
    ├── /docker                # Docker-related files
    │   └── docker-compose.yml # Docker Compose configuration
    ├── /scripts               # Any helper scripts (e.g., setup, maintenance)
    │   └── setup_environment.sh
    ├── .gitignore             # Git ignore file to exclude logs and other unwanted files
    └── README.md              # This file
```    
## Files and Folders
/dags: Contains your Airflow DAG files (e.g., simple_python_dag.py, bash_operator_dag.py).
/logs: Logs generated by Airflow (not tracked in Git).
/docker: Contains the docker-compose.yml file for setting up the environment.
/scripts: Contains helper scripts like setup_environment.sh that automate setup and container management.
.gitignore: Specifies files to ignore in Git (e.g., logs and environment files).
README.md: Documentation for setting up and using the project.
## Setup Instructions
1. Clone the Repository
Clone this repository to your local machine:

git clone [airflow demo ](https://github.com/renegadenz/apache-airflow-demo)
cd airflow-project
2. Make the Setup Script Executable
The setup_environment.sh script will set up your environment by creating necessary directories, starting Docker containers, and initializing Airflow.

Make the setup script executable:

```
chmod +x ./scripts/setup_environment.sh
```
3. Run the Setup Script
Execute the setup_environment.sh script to set up the Docker environment and start the services:

```
./scripts/setup_environment.sh
```
This will:

Create the dags and logs directories if they don't exist.
Start the Docker containers for Airflow and PostgreSQL.
Initialize the Airflow database.
4. Access the Airflow Web UI
Once the containers are up and running, open your browser and go to:

URL: http://localhost:8080
Username: airflow
Password: airflow
This will bring up the Airflow Web UI, where you can manage and monitor your DAGs.

5. Add or Modify DAGs
Add your Airflow DAG Python files to the dags folder. The Airflow containers will automatically detect and load these files.
You can add or modify DAGs at any time, and they will be picked up on the next scheduled run.
6. Stop the Airflow Environment
When you're done, you can stop and remove the containers with:

```
docker-compose down
```
This will stop all running containers and remove them.

7. Example DAGs
Two example DAGs have been provided to test the environment:

simple_python_dag.py: A simple DAG that runs a Python function to print a message.
bash_operator_dag.py: A DAG that runs a shell command (echo "Hello from BashOperator!").
Both of these DAGs are stored in the dags folder. You can modify these files or add new ones as needed.

8. Troubleshooting
Airflow Web UI is not loading: Ensure Docker is running, and all containers are up by running docker-compose ps.
DAGs not appearing in the UI: Check the logs (docker-compose logs airflow-webserver) to ensure that the DAGs are being loaded correctly.
Database initialization issues: Re-run the database initialization command with docker-compose exec airflow-webserver airflow db init.
